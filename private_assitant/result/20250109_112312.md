# Dialogue History

**You:** 把这段话翻译成中文，并且给出对这段话背后我的上司想表达的意思的提炼，尤其是告诉我他隐含的意思到底是支持我做这个项目还是不支持，如果我没法给出能够通过人力调整保证正确率的解决方案，他是否会对这个项目有消极看法：ここのロジックとしては、エンジニアの学習コストは高いので、生成AIを使ったコード自動生成での対策もあるが、その推論コストも高いのが課題。加えて、その確からしさにも課題が残るということで、実用性が低い。 なので、今回の開発によりその両課題を改善するソリューションを開発するということですよね。 この課題解決の中でのマルチエージェント化の位置付け（必要性）が今一つ分かっていないのと、KPIに関しては、「推論コスト低減」と「生成コードの確からしさ」というのが項目に上がってはいますが、そのスペック（数値）の根拠はなんらかあるのでしょうか。 この程度まで改善ができれば、あとは人が補正できる範囲という目安くらいですか？

**Agent:** ### 翻译：
这里的逻辑是，工程师的学习成本很高，所以虽然可以使用生成AI进行代码自动生成来应对，但其推理成本也很高，这是一个问题。此外，其准确性也存在问题，因此实用性较低。所以，通过这次开发，我们是要开发一个能够改善这两个问题的解决方案，对吧？在这个问题解决过程中，我对多代理化（必要性）的定位还不太清楚。关于KPI，虽然“推理成本降低”和“生成代码的准确性”被列为了项目目标，但这些指标（数值）的依据是什么呢？如果能够达到这种程度的改善，是否意味着剩下的部分可以通过人工修正来弥补？

### 提炼上司的隐含意思：
1. **对项目的支持态度**：上司对项目的整体方向是支持的，因为他提到了通过开发来改善现有的问题（推理成本高和准确性低）。这表明他认可项目的目标，并且希望通过技术手段来解决这些问题。

2. **对多代理化的疑虑**：上司对多代理化的必要性表示了一定的疑虑，认为其定位还不够清晰。这可能意味着他希望你在项目中对多代理化的作用进行更详细的解释或论证。

3. **对KPI的质疑**：上司对KPI的设定提出了疑问，特别是关于“推理成本降低”和“生成代码的准确性”的具体数值依据。这表明他希望看到更具体的、可量化的目标，而不是模糊的指标。

4. **对人工修正的期望**：上司提到“如果能够达到这种程度的改善，是否意味着剩下的部分可以通过人工修正来弥补”，这表明他可能认为完全依赖技术手段可能无法达到100%的准确性，因此人工修正可能是必要的。这也暗示了他对项目最终成果的期望是“可接受范围内的准确性”，而不是完美无缺。

### 隐含意思总结：
- **支持项目**：上司总体上支持这个项目，但希望看到更具体的解决方案和可量化的目标。
- **对多代理化的疑虑**：他对多代理化的必要性持保留态度，可能需要你进一步解释其价值。
- **对KPI的质疑**：他希望KPI有更明确的依据，而不是模糊的目标。
- **对人工修正的接受**：他可能认为完全依赖技术手段无法达到完美，因此人工修正是可以接受的。

### 结论：
如果你的解决方案无法通过人力调整保证正确率，上司可能会对项目的可行性产生一定的消极看法，但他并不完全反对项目，而是希望看到更具体的、可操作的解决方案。


**You:** # private_assistant_v2.py
# -*- coding: utf-8 -*-
import os
import logging
import time

from langchain.agents import Tool
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI

from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from google.api_core.exceptions import ResourceExhausted

# 配置日志记录
logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)

# ===== 1) 定义 LLM =====

#如果要使用 Google GenAI，请取消注释并配置好 GOOGLE_API_KEY
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key=os.environ["GOOGLE_API_KEY"]
)

# 这里使用 langchain 的 ChatOpenAI 初始化 deepseek_llm
deepseek_llm = ChatOpenAI(
    model='deepseek-chat',
    openai_api_key=os.environ["DEEPSEEK_API_KEY"],
    openai_api_base='https://api.deepseek.com',
    max_tokens=8192,
    temperature=0
)


# ===== 2) 定义是否继续对话的 Tool =====
def should_continue_tool_func(user_input):
    prompt = f"""判断用户是否明确表示要结束当前对话。仅回答yes或no，不包含任何其他文字。
yes表示用户不希望结束对话，no表示用户希望结束对话。
请结合用户的实际意图来判断，而不仅仅是根据关键词。
只有当用户非常明确地表示要结束对话时，才回答‘no’，否则回答‘yes’。

User Input: {user_input}

Response:"""
    response = deepseek_llm.invoke(prompt)
    return response.content.strip().lower() == "yes"


should_continue_tool = Tool(
    name="ShouldContinue",
    func=should_continue_tool_func,
    description="Determines whether the conversation should continue based on user input."
)


# ===== 3) 定义对话代理类 =====
class DialogueAgent:
    def __init__(self, model, system_prompt="You are a helpful assistant."):
        self.model = model
        self.system_prompt = system_prompt
        # 初始消息：SystemMessage
        self.messages = [SystemMessage(content=system_prompt)]
        self.dialogue_history = []  # 用于记录对话历史

    @retry(
        retry=retry_if_exception_type(ResourceExhausted),
        wait=wait_exponential(min=1, max=10),
        stop=stop_after_attempt(3),
        reraise=True
    )
    def interact_stream_generator(self, user_input):
        """
        以生成器形式流式输出的交互方法：
        - 前台 (GUI) 可以通过:
            for chunk in agent.interact_stream_generator(user_input):
                # 逐块更新UI
        - 最后一个 yield 之后，对话就完整结束
        """
        # 1) 记录用户输入
        self.messages.append(HumanMessage(content=user_input))
        self.dialogue_history.append(f"**You:** {user_input}")

        try:
            # 2) 调用模型的 stream 方法，得到一个可迭代的响应流
            response_stream = self.model.stream(self.messages)
            full_response = ""

            # 3) 逐块 yield 内容
            for chunk in response_stream:
                text_piece = chunk.content
                full_response += text_piece
                # 把当前块内容 yield 给前端
                yield text_piece

            # 4) 全部完成后，将完整响应写入对话历史
            self.messages.append(AIMessage(content=full_response))
            self.dialogue_history.append(f"**Agent:** {full_response}")

            # 5) 用 None 作为一个“结束标记”，告知前台收尾
            yield None

        except ResourceExhausted as e:
            logging.error(f"ResourceExhausted error: {e}")
            error_message = "API quota exhausted. Please try again later."
            self.dialogue_history.append(f"**Agent:** {error_message}")
            # 同样 yield 这个错误提示
            yield error_message
            yield None

        except Exception as e:
            logging.error(f"Error during interaction: {e}")
            error_message = "An error occurred during the interaction."
            self.dialogue_history.append(f"**Agent:** {error_message}")
            yield error_message
            yield None

    def save_dialogue_to_markdown(self, folder="result"):
        """
        将对话历史保存为 Markdown 文件
        """
        if not os.path.exists(folder):
            os.makedirs(folder)

        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = os.path.join(folder, f"{timestamp}.md")

        with open(filename, "w", encoding="utf-8") as file:
            file.write("# Dialogue History\n\n")
            for entry in self.dialogue_history:
                file.write(f"{entry}\n\n")

        logging.info(f"Dialogue saved to {filename}")

    # 可选：旧的非流式方法（命令行用）
    def interact(self, user_input):
        """
        不需要GUI的场景下，命令行一次性输出的简单示例。
        """
        self.messages.append(HumanMessage(content=user_input))
        self.dialogue_history.append(f"**You:** {user_input}")
        try:
            response_stream = self.model.stream(self.messages)
            print("Agent: ", end="", flush=True)
            full_response = ""
            for chunk in response_stream:
                print(chunk.content, end="", flush=True)
                full_response += chunk.content
            self.messages.append(AIMessage(content=full_response))
            self.dialogue_history.append(f"**Agent:** {full_response}")
            print()
            return full_response
        except ResourceExhausted as e:
            ...
        except Exception as e:
            ...
对这段程序，我要加一个新的tool，在我输入切换到deepseek或者切换到gemini的时候，帮我在llm和deepseek_llm之间切换。

**Agent:** 为了实现你提到的功能，我们需要添加一个新的 `Tool`，用于在 `llm` 和 `deepseek_llm` 之间切换。这个 `Tool` 会监听用户的输入，当用户输入“切换到deepseek”或“切换到gemini”时，动态切换当前使用的模型。

以下是实现这个功能的代码：

### 1. 定义切换模型的 Tool

```python
def switch_model_tool_func(user_input, agent):
    """
    根据用户输入切换模型。
    - 如果用户输入“切换到deepseek”，则切换到 deepseek_llm。
    - 如果用户输入“切换到gemini”，则切换到 llm（Gemini）。
    """
    if "切换到deepseek" in user_input.lower():
        agent.model = deepseek_llm
        return "已切换到 DeepSeek 模型。"
    elif "切换到gemini" in user_input.lower():
        agent.model = llm
        return "已切换到 Gemini 模型。"
    else:
        return "未检测到切换模型的指令。"
```

### 2. 创建 Tool 实例

```python
switch_model_tool = Tool(
    name="SwitchModel",
    func=lambda user_input: switch_model_tool_func(user_input, agent),  # 传入当前的 agent 实例
    description="Switches between DeepSeek and Gemini models based on user input."
)
```

### 3. 修改 `DialogueAgent` 类以支持模型切换

在 `DialogueAgent` 类中，我们需要确保 `model` 属性可以被动态修改。你已经在 `__init__` 中定义了 `self.model`，因此不需要额外修改。

### 4. 在对话中调用 Tool

在对话逻辑中，我们需要在每次用户输入时检查是否需要切换模型。可以在 `interact_stream_generator` 或 `interact` 方法中添加以下逻辑：

```python
def interact_stream_generator(self, user_input):
    # 检查是否需要切换模型
    switch_response = switch_model_tool_func(user_input, self)
    if "已切换到" in switch_response:
        yield switch_response  # 通知用户模型已切换
        return  # 结束当前交互

    # 继续正常的对话逻辑
    self.messages.append(HumanMessage(content=user_input))
    self.dialogue_history.append(f"**You:** {user_input}")

    try:
        response_stream = self.model.stream(self.messages)
        full_response = ""
        for chunk in response_stream:
            text_piece = chunk.content
            full_response += text_piece
            yield text_piece

        self.messages.append(AIMessage(content=full_response))
        self.dialogue_history.append(f"**Agent:** {full_response}")
        yield None

    except ResourceExhausted as e:
        logging.error(f"ResourceExhausted error: {e}")
        error_message = "API quota exhausted. Please try again later."
        self.dialogue_history.append(f"**Agent:** {error_message}")
        yield error_message
        yield None

    except Exception as e:
        logging.error(f"Error during interaction: {e}")
        error_message = "An error occurred during the interaction."
        self.dialogue_history.append(f"**Agent:** {error_message}")
        yield error_message
        yield None
```

### 5. 完整代码示例

以下是整合后的完整代码：

```python
# private_assistant_v2.py
# -*- coding: utf-8 -*-
import os
import logging
import time

from langchain.agents import Tool
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI

from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from google.api_core.exceptions import ResourceExhausted

# 配置日志记录
logging.basicConfig(level=logging.INFO)
logging.getLogger("httpx").setLevel(logging.WARNING)

# ===== 1) 定义 LLM =====

# 如果要使用 Google GenAI，请取消注释并配置好 GOOGLE_API_KEY
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key=os.environ["GOOGLE_API_KEY"]
)

# 这里使用 langchain 的 ChatOpenAI 初始化 deepseek_llm
deepseek_llm = ChatOpenAI(
    model='deepseek-chat',
    openai_api_key=os.environ["DEEPSEEK_API_KEY"],
    openai_api_base='https://api.deepseek.com',
    max_tokens=8192,
    temperature=0
)

# ===== 2) 定义是否继续对话的 Tool =====
def should_continue_tool_func(user_input):
    prompt = f"""判断用户是否明确表示要结束当前对话。仅回答yes或no，不包含任何其他文字。
yes表示用户不希望结束对话，no表示用户希望结束对话。
请结合用户的实际意图来判断，而不仅仅是根据关键词。
只有当用户非常明确地表示要结束对话时，才回答‘no’，否则回答‘yes’。

User Input: {user_input}

Response:"""
    response = deepseek_llm.invoke(prompt)
    return response.content.strip().lower() == "yes"

should_continue_tool = Tool(
    name="ShouldContinue",
    func=should_continue_tool_func,
    description="Determines whether the conversation should continue based on user input."
)

# ===== 3) 定义切换模型的 Tool =====
def switch_model_tool_func(user_input, agent):
    """
    根据用户输入切换模型。
    - 如果用户输入“切换到deepseek”，则切换到 deepseek_llm。
    - 如果用户输入“切换到gemini”，则切换到 llm（Gemini）。
    """
    if "切换到deepseek" in user_input.lower():
        agent.model = deepseek_llm
        return "已切换到 DeepSeek 模型。"
    elif "切换到gemini" in user_input.lower():
        agent.model = llm
        return "已切换到 Gemini 模型。"
    else:
        return "未检测到切换模型的指令。"

switch_model_tool = Tool(
    name="SwitchModel",
    func=lambda user_input: switch_model_tool_func(user_input, agent),  # 传入当前的 agent 实例
    description="Switches between DeepSeek and Gemini models based on user input."
)

# ===== 4) 定义对话代理类 =====
class DialogueAgent:
    def __init__(self, model, system_prompt="You are a helpful assistant."):
        self.model = model
        self.system_prompt = system_prompt
        # 初始消息：SystemMessage
        self.messages = [SystemMessage(content=system_prompt)]
        self.dialogue_history = []  # 用于记录对话历史

    @retry(
        retry=retry_if_exception_type(ResourceExhausted),
        wait=wait_exponential(min=1, max=10),
        stop=stop_after_attempt(3),
        reraise=True
    )
    def interact_stream_generator(self, user_input):
        """
        以生成器形式流式输出的交互方法：
        - 前台 (GUI) 可以通过:
            for chunk in agent.interact_stream_generator(user_input):
                # 逐块更新UI
        - 最后一个 yield 之后，对话就完整结束
        """
        # 检查是否需要切换模型
        switch_response = switch_model_tool_func(user_input, self)
        if "已切换到" in switch_response:
            yield switch_response  # 通知用户模型已切换
            return  # 结束当前交互

        # 1) 记录用户输入
        self.messages.append(HumanMessage(content=user_input))
        self.dialogue_history.append(f"**You:** {user_input}")

        try:
            # 2) 调用模型的 stream 方法，得到一个可迭代的响应流
            response_stream = self.model.stream(self.messages)
            full_response = ""

            # 3) 逐块 yield 内容
            for chunk in response_stream:
                text_piece = chunk.content
                full_response += text_piece
                # 把当前块内容 yield 给前端
                yield text_piece

            # 4) 全部完成后，将完整响应写入对话历史
            self.messages.append(AIMessage(content=full_response))
            self.dialogue_history.append(f"**Agent:** {full_response}")

            # 5) 用 None 作为一个“结束标记”，告知前台收尾
            yield None

        except ResourceExhausted as e:
            logging.error(f"ResourceExhausted error: {e}")
            error_message = "API quota exhausted. Please try again later."
            self.dialogue_history.append(f"**Agent:** {error_message}")
            # 同样 yield 这个错误提示
            yield error_message
            yield None

        except Exception as e:
            logging.error(f"Error during interaction: {e}")
            error_message = "An error occurred during the interaction."
            self.dialogue_history.append(f"**Agent:** {error_message}")
            yield error_message
            yield None

    def save_dialogue_to_markdown(self, folder="result"):
        """
        将对话历史保存为 Markdown 文件
        """
        if not os.path.exists(folder):
            os.makedirs(folder)

        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = os.path.join(folder, f"{timestamp}.md")

        with open(filename, "w", encoding="utf-8") as file:
            file.write("# Dialogue History\n\n")
            for entry in self.dialogue_history:
                file.write(f"{entry}\n\n")

        logging.info(f"Dialogue saved to {filename}")

    # 可选：旧的非流式方法（命令行用）
    def interact(self, user_input):
        """
        不需要GUI的场景下，命令行一次性输出的简单示例。
        """
        self.messages.append(HumanMessage(content=user_input))
        self.dialogue_history.append(f"**You:** {user_input}")
        try:
            response_stream = self.model.stream(self.messages)
            print("Agent: ", end="", flush=True)
            full_response = ""
            for chunk in response_stream:
                print(chunk.content, end="", flush=True)
                full_response += chunk.content
            self.messages.append(AIMessage(content=full_response))
            self.dialogue_history.append(f"**Agent:** {full_response}")
            print()
            return full_response
        except ResourceExhausted as e:
            ...
        except Exception as e:
            ...
```

### 6. 使用示例

- 用户输入“切换到deepseek”时，模型会切换到 `deepseek_llm`。
- 用户输入“切换到gemini”时，模型会切换到 `llm`（Gemini）。

### 7. 注意事项

- 确保 `agent` 实例在 `switch_model_tool` 中正确传递。
- 如果需要在多个地方调用 `switch_model_tool`，可以将 `agent` 实例作为参数传递，或者将其作为类属性。

通过这种方式，你可以轻松地在对话中切换模型，而无需重启程序或重新初始化。



**You:** import os
import ast
import json
import sys
import importlib
import importlib.metadata

def extract_ast_features_from_file(file_path, project_root):
    with open(file_path, 'r', encoding='utf-8') as f:
        code = f.read()

    try:
        tree = ast.parse(code)
    except SyntaxError:
        return None

    features = {
        "file_path": file_path,
        "used_functions": [],
        "imports": []
    }

    imported_modules = {}

    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                imported_modules[alias.name] = alias.asname or alias.name
                features["imports"].append({
                    "import_name": alias.name,
                    "from_name": None,
                    "as_name": alias.asname,
                    "source": classify_import_source(alias.name, project_root)
                })
        elif isinstance(node, ast.ImportFrom):
            for alias in node.names:
                imported_modules[f"{node.module}.{alias.name}"] = alias.asname or alias.name
                features["imports"].append({
                    "import_name": alias.name,
                    "from_name": node.module,
                    "as_name": alias.asname,
                    "source": classify_import_source(node.module, project_root)
                })
        elif isinstance(node, ast.Call):
            func_name = ""
            if isinstance(node.func, ast.Name):
                func_name = node.func.id
            elif isinstance(node.func, ast.Attribute):
                if isinstance(node.func.value, ast.Name):
                    module_name = node.func.value.id
                    if module_name in imported_modules:
                        func_name = f"{imported_modules[module_name]}.{node.func.attr}"
                    else:
                        func_name = f"{module_name}.{node.func.attr}"
                elif isinstance(node.func.value,ast.Call):
                    #处理a().b()的情况，递归调用
                    func_name_prefix=extract_function_name_from_node(node.func.value,imported_modules)
                    if func_name_prefix:
                        func_name = f"{func_name_prefix}.{node.func.attr}"
                else:
                    pass
            elif isinstance(node.func, ast.Subscript):
                #处理a[0]()的情况
                pass
            if func_name:
                features["used_functions"].append(func_name)

    return features
def extract_function_name_from_node(node,imported_modules):
    func_name = ""
    if isinstance(node, ast.Name):
        func_name = node.id
    elif isinstance(node, ast.Attribute):
        if isinstance(node.func.value, ast.Name):
            module_name = node.func.value.id
            if module_name in imported_modules:
                func_name = f"{imported_modules[module_name]}.{node.func.attr}"
            else:
                func_name = f"{module_name}.{node.func.attr}"
        elif isinstance(node.func.value,ast.Call):
            func_name_prefix=extract_function_name_from_node(node.func.value,imported_modules)
            if func_name_prefix:
                func_name = f"{func_name_prefix}.{node.func.attr}"
        else:
            pass
    return func_name

def classify_import_source(module_name, project_root):
    if module_name in sys.builtin_module_names:
        return "内置模块"

    try:
        installed_packages = {pkg.metadata['Name'].lower() for pkg in importlib.metadata.distributions()}
        if module_name.lower() in installed_packages:
            return "第三方库"
    except Exception as e:
        print(f"Error checking installed packages: {e}")
        return "未知模块"

    local_path = os.path.join(project_root, module_name.replace('.', os.sep) + '.py')
    if os.path.exists(local_path):
        return f"本地文件: {local_path}"

    return "未知模块"

def traverse_directory_and_extract_features(directory):
    ast_features = []

    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".py"):
                file_path = os.path.join(root, file)
                features = extract_ast_features_from_file(file_path, directory)
                if features:
                    ast_features.append(features)

    return ast_features

def save_features_to_json(features, output_path):
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(features, f, indent=4, ensure_ascii=False)

if __name__ == "__main__":
    project_directory = "Python"  # 替换为你的项目路径
    output_file = "ast_features.json"

    print(f"正在解析项目目录：{project_directory}")
    features = traverse_directory_and_extract_features(project_directory)
    print(f"解析完成，共提取 {len(features)} 个文件的AST特征。")

    save_features_to_json(features, output_file)
    print(f"AST特征已保存至：{output_file}")对这段程序，对于每个py文件，以它的每个函数为界限分析他会使用到的库，而不是整个py文件会使用到的库

**Agent:** 要实现以每个函数为界限分析其使用的库，我们需要对代码进行更细粒度的分析。具体来说，我们需要遍历每个函数的 AST 节点，并记录该函数内部使用的库和函数调用。

以下是修改后的代码，实现了以函数为单位分析其使用的库：

### 修改后的代码

```python
import os
import ast
import json
import sys
import importlib
import importlib.metadata

def extract_ast_features_from_file(file_path, project_root):
    with open(file_path, 'r', encoding='utf-8') as f:
        code = f.read()

    try:
        tree = ast.parse(code)
    except SyntaxError:
        return None

    features = {
        "file_path": file_path,
        "functions": []
    }

    imported_modules = {}

    # 先提取全局的 import 信息
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for alias in node.names:
                imported_modules[alias.name] = alias.asname or alias.name
        elif isinstance(node, ast.ImportFrom):
            for alias in node.names:
                imported_modules[f"{node.module}.{alias.name}"] = alias.asname or alias.name

    # 遍历每个函数定义
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            function_features = {
                "function_name": node.name,
                "used_functions": [],
                "imports": []
            }

            # 遍历函数体内的节点
            for func_node in ast.walk(node):
                if isinstance(func_node, ast.Call):
                    func_name = extract_function_name_from_node(func_node.func, imported_modules)
                    if func_name:
                        function_features["used_functions"].append(func_name)

            # 记录该函数使用的库
            for import_name, as_name in imported_modules.items():
                for used_func in function_features["used_functions"]:
                    if used_func.startswith(import_name):
                        function_features["imports"].append({
                            "import_name": import_name,
                            "as_name": as_name,
                            "source": classify_import_source(import_name, project_root)
                        })

            features["functions"].append(function_features)

    return features

def extract_function_name_from_node(node, imported_modules):
    func_name = ""
    if isinstance(node, ast.Name):
        func_name = node.id
    elif isinstance(node, ast.Attribute):
        if isinstance(node.value, ast.Name):
            module_name = node.value.id
            if module_name in imported_modules:
                func_name = f"{imported_modules[module_name]}.{node.attr}"
            else:
                func_name = f"{module_name}.{node.attr}"
        elif isinstance(node.value, ast.Call):
            func_name_prefix = extract_function_name_from_node(node.value, imported_modules)
            if func_name_prefix:
                func_name = f"{func_name_prefix}.{node.attr}"
    return func_name

def classify_import_source(module_name, project_root):
    if module_name in sys.builtin_module_names:
        return "内置模块"

    try:
        installed_packages = {pkg.metadata['Name'].lower() for pkg in importlib.metadata.distributions()}
        if module_name.lower() in installed_packages:
            return "第三方库"
    except Exception as e:
        print(f"Error checking installed packages: {e}")
        return "未知模块"

    local_path = os.path.join(project_root, module_name.replace('.', os.sep) + '.py')
    if os.path.exists(local_path):
        return f"本地文件: {local_path}"

    return "未知模块"

def traverse_directory_and_extract_features(directory):
    ast_features = []

    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".py"):
                file_path = os.path.join(root, file)
                features = extract_ast_features_from_file(file_path, directory)
                if features:
                    ast_features.append(features)

    return ast_features

def save_features_to_json(features, output_path):
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(features, f, indent=4, ensure_ascii=False)

if __name__ == "__main__":
    project_directory = "Python"  # 替换为你的项目路径
    output_file = "ast_features.json"

    print(f"正在解析项目目录：{project_directory}")
    features = traverse_directory_and_extract_features(project_directory)
    print(f"解析完成，共提取 {len(features)} 个文件的AST特征。")

    save_features_to_json(features, output_file)
    print(f"AST特征已保存至：{output_file}")
```

### 主要修改点

1. **函数级别的分析**：
   - 在 `extract_ast_features_from_file` 函数中，我们遍历每个 `ast.FunctionDef` 节点，提取函数名和函数体内的调用信息。
   - 每个函数的 `used_functions` 和 `imports` 信息被单独记录。

2. **函数调用解析**：
   - `extract_function_name_from_node` 函数用于解析函数调用节点，提取完整的函数名（包括模块名和函数名）。

3. **库的使用分析**：
   - 在函数级别，我们检查每个函数调用的函数是否依赖于某个导入的库，并将相关信息记录在 `imports` 中。

4. **输出结构**：
   - 每个文件的输出结构包含多个函数的信息，每个函数的信息包括其使用的库和调用的函数。

### 输出示例

假设有一个 Python 文件 `example.py`，内容如下：

```python
import os
import math

def func1():
    print(os.getcwd())
    return math.sqrt(16)

def func2():
    return os.path.join("a", "b")
```

运行程序后，输出的 JSON 文件内容可能如下：

```json
[
    {
        "file_path": "example.py",
        "functions": [
            {
                "function_name": "func1",
                "used_functions": ["os.getcwd", "math.sqrt"],
                "imports": [
                    {
                        "import_name": "os",
                        "as_name": "os",
                        "source": "内置模块"
                    },
                    {
                        "import_name": "math",
                        "as_name": "math",
                        "source": "内置模块"
                    }
                ]
            },
            {
                "function_name": "func2",
                "used_functions": ["os.path.join"],
                "imports": [
                    {
                        "import_name": "os",
                        "as_name": "os",
                        "source": "内置模块"
                    }
                ]
            }
        ]
    }
]
```

### 总结

通过这种方式，你可以以函数为单位分析其使用的库和函数调用，从而更精确地了解代码的依赖关系。这对于代码优化、依赖管理和重构非常有帮助。

